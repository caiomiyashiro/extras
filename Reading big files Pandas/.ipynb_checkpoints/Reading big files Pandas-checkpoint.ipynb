{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/pandas-big-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is on S3: mytaxi-datascience-passenger-destination/data/processed/sorted_passenger_tours_processed.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/caiomiyashiro/repo/passenger_destination/data/raw/last_16_passenger_tours_v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much space does this dataframe use? (17.2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Internal Representation of a Dataframe?\n",
    "  \n",
    "Under the hood, a Pandas Data Frame stores all the contiguous values in different sub data structures. For numeric data structures, each sub-block is stored in a numpy array, which per se are wrappers for C arrays style, which finally makes it efficient to access and process.\n",
    "\n",
    "The same doesn't happen for object blocks, which can be anything else besides numeric. With this, they're subject to all the Python slowliness it is subject to (Source: [Why Python is Slow?](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/))\n",
    "\n",
    "<img src=\"img1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what is the average column memory size for our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_memory_use(df, dtypes=['float','int','object']):\n",
    "    for dtype in dtypes:\n",
    "        selected_dtype = df.select_dtypes(include=[dtype])\n",
    "        mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "        mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "        print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n",
    "        \n",
    "average_memory_use(df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some stuff we can do to decrease the data size:\n",
    "\n",
    "## Defining function to extract the column's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "original_df_size = mem_usage(df)\n",
    "print('Original data frame memory size: ' + str(original_df_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pd.to_numeric() to downcast numeric variables - Floats or Integers\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df.select_dtypes(include=['float'])\n",
    "df_float_converted = df_float.apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "print('Memory used by all the float64:')\n",
    "print(mem_usage(df_float))\n",
    "print('\\n')\n",
    "print('New memory size used by all the float32:')\n",
    "print(mem_usage(df_float_converted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why string objects take so much space?\n",
    "  \n",
    "  \n",
    "Every string present in the dataframe will be repetively stored in the computer's memory, *i.e.* if I have a columns of 1 million rows with a string value in it, python will store 1 million string on memory, even though it could store just a reference to same place in memory.\n",
    "  \n",
    "\n",
    "<img src=\"numpy_vs_python.png\" width=\"600\">\n",
    "source: https://www.dataquest.io/blog/pandas-big-data/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting string objects to category\n",
    "\n",
    "[The category dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html) does exactly the above, it creates a numeric representation of each category value and just store in the array a reference to each value. This is the **main** memory save **if** we have a limited amount of category values of course. If we convert an id columns, it will at the end just increase the memory size because it had to create an numeric mapping and still keep all the distinct category values in memory.  \n",
    "  \n",
    "A rule of thumb (totally) is to turn variables in category if the amount of distinct values <= 50% of the numbers of rows. Keeping like this it is more probable we end up saving memory :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object = df.select_dtypes(include=['object']).copy()\n",
    "# we don't want the convert date_created to category, we want to keep it as datetime\n",
    "df_object.drop('date_created', axis=1,inplace=True) \n",
    "\n",
    "df_object_converted = df_object.apply(lambda col: col.astype('category'))\n",
    "\n",
    "print('Memory used by all the object dtypes:')\n",
    "print(mem_usage(df_object))\n",
    "print('\\n')\n",
    "print('New memory size used by all the category dtypes:')\n",
    "print(mem_usage(df_object_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0757753240005503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1134.64/14973.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is equal to > 90% memory reduction!\n",
    "  \n",
    "Below we can see the transformation is just under the hood. Up front, the data is 100% the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object_converted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Every Transformation\n",
    "\n",
    "Lets take the original dataset, apply the transformations and compare the old with the new total size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df_float_converted.columns] = df_float_converted\n",
    "df[df_object_converted.columns] = df_object_converted\n",
    "\n",
    "new_df_size = mem_usage(df)\n",
    "\n",
    "print('Original data frame memory size: ' + str(original_df_size))\n",
    "print('New data frame memory size: ' + str(new_df_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# During Projects\n",
    "\n",
    "If the dataset is already too big, we wouldn't be able to read the whole file so just then decrease its size. An approach that I did was to read in a limited amount of lines and process the lines in order to identify the objects dtypes, as they are the most useless in terms of memory usage. For that we use the **nrows** parameter and them loop create a dictionary with the columns' names and their dtypes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_passenger</th>\n",
       "      <th>date_created</th>\n",
       "      <th>id</th>\n",
       "      <th>request_long</th>\n",
       "      <th>request_lat</th>\n",
       "      <th>request_street</th>\n",
       "      <th>request_street_number</th>\n",
       "      <th>dest_long</th>\n",
       "      <th>dest_lat</th>\n",
       "      <th>dest_street</th>\n",
       "      <th>...</th>\n",
       "      <th>lag10_street</th>\n",
       "      <th>lag10_street_number</th>\n",
       "      <th>home_dest_long</th>\n",
       "      <th>home_dest_lat</th>\n",
       "      <th>home_street_name</th>\n",
       "      <th>home_street_number</th>\n",
       "      <th>work_dest_long</th>\n",
       "      <th>work_dest_lat</th>\n",
       "      <th>work_street_name</th>\n",
       "      <th>work_street_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5529</td>\n",
       "      <td>2017-01-03 23:34:08.724</td>\n",
       "      <td>34667961</td>\n",
       "      <td>13.43457</td>\n",
       "      <td>52.52301</td>\n",
       "      <td>Landsberger Allee</td>\n",
       "      <td>4</td>\n",
       "      <td>13.38814</td>\n",
       "      <td>52.51476</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5783</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>13.387784</td>\n",
       "      <td>52.514698</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5529</td>\n",
       "      <td>2017-01-19 07:00:10.124</td>\n",
       "      <td>35328512</td>\n",
       "      <td>13.38778</td>\n",
       "      <td>52.51470</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "      <td>13.29529</td>\n",
       "      <td>52.55517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5783</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>13.387784</td>\n",
       "      <td>52.514698</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5529</td>\n",
       "      <td>2017-04-02 17:53:14.823</td>\n",
       "      <td>39716974</td>\n",
       "      <td>11.57830</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>11.78946</td>\n",
       "      <td>48.35712</td>\n",
       "      <td>Terminalstraße Ost</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5783</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>13.387784</td>\n",
       "      <td>52.514698</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5529</td>\n",
       "      <td>2017-08-12 13:41:55.805</td>\n",
       "      <td>53875521</td>\n",
       "      <td>9.96889</td>\n",
       "      <td>53.55451</td>\n",
       "      <td>Heiligengeistfeld</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.98529</td>\n",
       "      <td>53.54142</td>\n",
       "      <td>Platz der Deutschen Einheit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5783</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>13.387784</td>\n",
       "      <td>52.514698</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5529</td>\n",
       "      <td>2017-09-04 16:23:47.750</td>\n",
       "      <td>56454276</td>\n",
       "      <td>10.02013</td>\n",
       "      <td>53.55710</td>\n",
       "      <td>Steindamm</td>\n",
       "      <td>105</td>\n",
       "      <td>9.96422</td>\n",
       "      <td>53.55922</td>\n",
       "      <td>Neuer Pferdemarkt</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5783</td>\n",
       "      <td>48.11313</td>\n",
       "      <td>Perlacher Straße</td>\n",
       "      <td>8</td>\n",
       "      <td>13.387784</td>\n",
       "      <td>52.514698</td>\n",
       "      <td>Französische Straße</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_passenger             date_created        id  request_long  request_lat  \\\n",
       "0          5529  2017-01-03 23:34:08.724  34667961      13.43457     52.52301   \n",
       "1          5529  2017-01-19 07:00:10.124  35328512      13.38778     52.51470   \n",
       "2          5529  2017-04-02 17:53:14.823  39716974      11.57830     48.11313   \n",
       "3          5529  2017-08-12 13:41:55.805  53875521       9.96889     53.55451   \n",
       "4          5529  2017-09-04 16:23:47.750  56454276      10.02013     53.55710   \n",
       "\n",
       "        request_street request_street_number  dest_long  dest_lat  \\\n",
       "0    Landsberger Allee                     4   13.38814  52.51476   \n",
       "1  Französische Straße                    55   13.29529  52.55517   \n",
       "2     Perlacher Straße                     8   11.78946  48.35712   \n",
       "3    Heiligengeistfeld                   NaN    9.98529  53.54142   \n",
       "4            Steindamm                   105    9.96422  53.55922   \n",
       "\n",
       "                   dest_street        ...         lag10_street  \\\n",
       "0          Französische Straße        ...                  NaN   \n",
       "1                          NaN        ...                  NaN   \n",
       "2           Terminalstraße Ost        ...                  NaN   \n",
       "3  Platz der Deutschen Einheit        ...                  NaN   \n",
       "4            Neuer Pferdemarkt        ...                  NaN   \n",
       "\n",
       "   lag10_street_number  home_dest_long  home_dest_lat  home_street_name  \\\n",
       "0                  NaN         11.5783       48.11313  Perlacher Straße   \n",
       "1                  NaN         11.5783       48.11313  Perlacher Straße   \n",
       "2                  NaN         11.5783       48.11313  Perlacher Straße   \n",
       "3                  NaN         11.5783       48.11313  Perlacher Straße   \n",
       "4                  NaN         11.5783       48.11313  Perlacher Straße   \n",
       "\n",
       "  home_street_number  work_dest_long  work_dest_lat     work_street_name  \\\n",
       "0                  8       13.387784      52.514698  Französische Straße   \n",
       "1                  8       13.387784      52.514698  Französische Straße   \n",
       "2                  8       13.387784      52.514698  Französische Straße   \n",
       "3                  8       13.387784      52.514698  Französische Straße   \n",
       "4                  8       13.387784      52.514698  Französische Straße   \n",
       "\n",
       "  work_street_number  \n",
       "0                 55  \n",
       "1                 55  \n",
       "2                 55  \n",
       "3                 55  \n",
       "4                 55  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read only a sample to detect data types\n",
    "df = pd.read_csv('/Users/caiomiyashiro/repo/passenger_destination/data/raw/last_16_passenger_tours_v2.csv', index_col=0, nrows=10000)\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dest_street': 'category',\n",
       " 'dest_street_number': 'category',\n",
       " 'home_street_name': 'category',\n",
       " 'home_street_number': 'category',\n",
       " 'lag10_street': 'category',\n",
       " 'lag10_street_number': 'category',\n",
       " 'lag1_street': 'category',\n",
       " 'lag1_street_number': 'category',\n",
       " 'lag2_street': 'category',\n",
       " 'lag2_street_number': 'category',\n",
       " 'lag3_street': 'category',\n",
       " 'lag3_street_number': 'category',\n",
       " 'lag4_street': 'category',\n",
       " 'lag4_street_number': 'category',\n",
       " 'lag5_street': 'category',\n",
       " 'lag5_street_number': 'category',\n",
       " 'lag6_street': 'category',\n",
       " 'lag6_street_number': 'category',\n",
       " 'lag7_street': 'category',\n",
       " 'lag7_street_number': 'category',\n",
       " 'lag8_street': 'category',\n",
       " 'lag8_street_number': 'category',\n",
       " 'lag9_street': 'category',\n",
       " 'lag9_street_number': 'category',\n",
       " 'request_street': 'category',\n",
       " 'request_street_number': 'category',\n",
       " 'work_street_name': 'category',\n",
       " 'work_street_number': 'category'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes_df = {}\n",
    "obj_cols = df.select_dtypes(include=['object']).columns[1:].tolist() # [1:] is just to skip the 'date_created'\n",
    "for obj_col in obj_cols:\n",
    "    if(len(df[obj_col].drop_duplicates()) < df.shape[0]/2):\n",
    "        dtypes_df[obj_col] = 'category'\n",
    "dtypes_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the created dictionary as input for the **pd.read_csv** function in the **dtype** parameter. Pandas won't try to infer the column's type and directly try to apply the specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predefined formats data frame memory size: 3225.63 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/sorted_passenger_tours_v2.csv', index_col=0, \n",
    "                 dtype=dtypes_df, parse_dates=['date_created'], infer_datetime_format=True)\n",
    "\n",
    "print('Predefined formats data frame memory size: ' + str(mem_usage(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things I know it doesn't work and other questions:\n",
    "\n",
    "- Unfortunately, when working with pandas, I know the category data type is not automatically interpreted as factors, like in R. I haven't found a solution for this besides using again lots of memory to convert the category dtypes to strings (pandas will do this when calling pd.get_dummies() anyway)\n",
    "  \n",
    "- If we have to deal with string manipulation, I don't know if the category dtype impact in the performance.  \n",
    "  \n",
    "- In case of nominal data where the categories have an order, *e.g.* 'disagree', 'agree', 'strongly agree', we can instantiate the categorical dtype in a similar way, using one extra parameter of *order = True* [Link here](https://pandas.pydata.org/pandas-docs/stable/categorical.html#controlling-behavior)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
