{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/pandas-big-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fictional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (79,83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Library/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('*.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much space does this dataframe use? (17.2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8581762 entries, 2747 to 31942516\n",
      "Data columns (total 83 columns):\n",
      "date_created                   object\n",
      "id                             int64\n",
      "request_long                   float64\n",
      "request_lat                    float64\n",
      "request_street                 object\n",
      "request_street_number          object\n",
      "dest_long                      float64\n",
      "dest_lat                       float64\n",
      "dest_street                    object\n",
      "dest_street_number             object\n",
      "passenger_input_destination    bool\n",
      "lag1_dest_long                 float64\n",
      "lag1_dest_lat                  float64\n",
      "lag1_street                    object\n",
      "lag1_street_number             object\n",
      "lag2_dest_long                 float64\n",
      "lag2_dest_lat                  float64\n",
      "lag2_street                    object\n",
      "lag2_street_number             object\n",
      "lag3_dest_long                 float64\n",
      "lag3_dest_lat                  float64\n",
      "lag3_street                    object\n",
      "lag3_street_number             object\n",
      "lag4_dest_long                 float64\n",
      "lag4_dest_lat                  float64\n",
      "lag4_street                    object\n",
      "lag4_street_number             object\n",
      "lag5_dest_long                 float64\n",
      "lag5_dest_lat                  float64\n",
      "lag5_street                    object\n",
      "lag5_street_number             object\n",
      "lag6_dest_long                 float64\n",
      "lag6_dest_lat                  float64\n",
      "lag6_street                    object\n",
      "lag6_street_number             object\n",
      "lag7_dest_long                 float64\n",
      "lag7_dest_lat                  float64\n",
      "lag7_street                    object\n",
      "lag7_street_number             object\n",
      "lag8_dest_long                 float64\n",
      "lag8_dest_lat                  float64\n",
      "lag8_street                    object\n",
      "lag8_street_number             object\n",
      "lag9_dest_long                 float64\n",
      "lag9_dest_lat                  float64\n",
      "lag9_street                    object\n",
      "lag9_street_number             object\n",
      "lag10_dest_long                float64\n",
      "lag10_dest_lat                 float64\n",
      "lag10_street                   object\n",
      "lag10_street_number            object\n",
      "lag11_dest_long                float64\n",
      "lag11_dest_lat                 float64\n",
      "lag11_street                   object\n",
      "lag11_street_number            object\n",
      "lag12_dest_long                float64\n",
      "lag12_dest_lat                 float64\n",
      "lag12_street                   object\n",
      "lag12_street_number            object\n",
      "lag13_dest_long                float64\n",
      "lag13_dest_lat                 float64\n",
      "lag13_street                   object\n",
      "lag13_street_number            object\n",
      "lag14_dest_long                float64\n",
      "lag14_dest_lat                 float64\n",
      "lag14_street                   object\n",
      "lag14_street_number            object\n",
      "lag15_dest_long                float64\n",
      "lag15_dest_lat                 float64\n",
      "lag15_street                   object\n",
      "lag15_street_number            object\n",
      "lag16_dest_long                float64\n",
      "lag16_dest_lat                 float64\n",
      "lag16_street                   object\n",
      "lag16_street_number            object\n",
      "home_dest_long                 float64\n",
      "home_dest_lat                  float64\n",
      "home_street_name               object\n",
      "home_street_number             object\n",
      "work_dest_long                 float64\n",
      "work_dest_lat                  float64\n",
      "work_street_name               object\n",
      "work_street_number             object\n",
      "dtypes: bool(1), float64(40), int64(1), object(41)\n",
      "memory usage: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Internal Representation of a Dataframe?\n",
    "  \n",
    "Under the hood, a Pandas Data Frame stores all the contiguous values in different sub data structures. For numeric data structures, each sub-block is stored in a numpy array, which per se are wrappers for C arrays style, which finally makes it efficient to access and process.\n",
    "\n",
    "The same doesn't happen for object blocks, which can be anything else besides numeric. With this, they're subject to all the Python slowliness it is subject to (Source: [Why Python is Slow?](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/))\n",
    "\n",
    "<img src=\"img1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what is the average column memory size for our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage for float columns: 65.47 MB\n",
      "Average memory usage for int columns: 65.47 MB\n",
      "Average memory usage for object columns: 512.24 MB\n"
     ]
    }
   ],
   "source": [
    "def average_memory_use(df, dtypes=['float','int','object']):\n",
    "    for dtype in dtypes:\n",
    "        selected_dtype = df.select_dtypes(include=[dtype])\n",
    "        mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "        mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "        print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n",
    "        \n",
    "average_memory_use(df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some stuff we can do to decrease the data size:\n",
    "\n",
    "## Defining function to extract the column's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data frame memory size: 24206.56 MB\n"
     ]
    }
   ],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "\n",
    "original_df_size = mem_usage(df)\n",
    "print('Original data frame memory size: ' + str(original_df_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pd.to_numeric() to downcast numeric variables - Floats or Integers\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by all the float64:\n",
      "2684.42 MB\n",
      "\n",
      "\n",
      "New memory size used by all the float32:\n",
      "1374.95 MB\n"
     ]
    }
   ],
   "source": [
    "df_float = df.select_dtypes(include=['float'])\n",
    "df_float_converted = df_float.apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "print('Memory used by all the float64:')\n",
    "print(mem_usage(df_float))\n",
    "print('\\n')\n",
    "print('New memory size used by all the float32:')\n",
    "print(mem_usage(df_float_converted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why string objects take so much space?\n",
    "  \n",
    "  \n",
    "Every string present in the dataframe will be repetively stored in the computer's memory, *i.e.* if I have a columns of 1 million rows with a string value in it, python will store 1 million string on memory, even though it could store just a reference to same place in memory.\n",
    "  \n",
    "\n",
    "<img src=\"numpy_vs_python.png\" width=\"600\">\n",
    "source: https://www.dataquest.io/blog/pandas-big-data/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting string objects to category\n",
    "\n",
    "[The category dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html) does exactly the above, it creates a numeric representation of each category value and just store in the array a reference to each value. This is the **main** memory save **if** we have a limited amount of category values of course. If we convert an id columns, it will at the end just increase the memory size because it had to create an numeric mapping and still keep all the distinct category values in memory.  \n",
    "  \n",
    "A rule of thumb (totally) is to turn variables in category if the amount of distinct values <= 50% of the numbers of rows. Keeping like this it is more probable we end up saving memory :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_created', 'request_street', 'request_street_number',\n",
       "       'dest_street', 'dest_street_number', 'lag1_street',\n",
       "       'lag1_street_number', 'lag2_street', 'lag2_street_number',\n",
       "       'lag3_street', 'lag3_street_number', 'lag4_street',\n",
       "       'lag4_street_number', 'lag5_street', 'lag5_street_number',\n",
       "       'lag6_street', 'lag6_street_number', 'lag7_street',\n",
       "       'lag7_street_number', 'lag8_street', 'lag8_street_number',\n",
       "       'lag9_street', 'lag9_street_number', 'lag10_street',\n",
       "       'lag10_street_number', 'lag11_street', 'lag11_street_number',\n",
       "       'lag12_street', 'lag12_street_number', 'lag13_street',\n",
       "       'lag13_street_number', 'lag14_street', 'lag14_street_number',\n",
       "       'lag15_street', 'lag15_street_number', 'lag16_street',\n",
       "       'lag16_street_number', 'home_street_name', 'home_street_number',\n",
       "       'work_street_name', 'work_street_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by all the object dtypes:\n",
      "20891.96 MB\n",
      "\n",
      "\n",
      "New memory size used by all the category dtypes:\n",
      "1580.18 MB\n"
     ]
    }
   ],
   "source": [
    "df_object = df.select_dtypes(include=['object']).copy()\n",
    "# we don't want the convert date_created to category, we want to keep it as datetime\n",
    "df_object.drop('date_created', axis=1,inplace=True) \n",
    "\n",
    "df_object_converted = df_object.apply(lambda col: col.astype('category'))\n",
    "\n",
    "print('Memory used by all the object dtypes:')\n",
    "print(mem_usage(df_object))\n",
    "print('\\n')\n",
    "print('New memory size used by all the category dtypes:')\n",
    "print(mem_usage(df_object_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0757753240005503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1134.64/14973.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is equal to > 90% memory reduction!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Every Transformation\n",
    "\n",
    "Lets take the original dataset, apply the transformations and compare the old with the new total size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data frame memory size: 24206.56 MB\n",
      "New data frame memory size: 3585.31 MB\n"
     ]
    }
   ],
   "source": [
    "df[df_float_converted.columns] = df_float_converted\n",
    "df[df_object_converted.columns] = df_object_converted\n",
    "\n",
    "new_df_size = mem_usage(df)\n",
    "\n",
    "print('Original data frame memory size: ' + str(original_df_size))\n",
    "print('New data frame memory size: ' + str(new_df_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# During Projects\n",
    "\n",
    "If the dataset is already too big, we wouldn't be able to read the whole file so just then decrease its size. An approach that I did was to read in a limited amount of lines and process the lines in order to identify the objects dtypes, as they are the most useless in terms of memory usage. For that we use the **nrows** parameter and them loop create a dictionary with the columns' names and their dtypes :\n",
    "\n",
    "- `Read only a sample to detect data types` - Deleted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dest_street': 'category',\n",
       " 'dest_street_number': 'category',\n",
       " 'home_street_name': 'category',\n",
       " 'home_street_number': 'category',\n",
       " 'lag10_street': 'category',\n",
       " 'lag10_street_number': 'category',\n",
       " 'lag1_street': 'category',\n",
       " 'lag1_street_number': 'category',\n",
       " 'lag2_street': 'category',\n",
       " 'lag2_street_number': 'category',\n",
       " 'lag3_street': 'category',\n",
       " 'lag3_street_number': 'category',\n",
       " 'lag4_street': 'category',\n",
       " 'lag4_street_number': 'category',\n",
       " 'lag5_street': 'category',\n",
       " 'lag5_street_number': 'category',\n",
       " 'lag6_street': 'category',\n",
       " 'lag6_street_number': 'category',\n",
       " 'lag7_street': 'category',\n",
       " 'lag7_street_number': 'category',\n",
       " 'lag8_street': 'category',\n",
       " 'lag8_street_number': 'category',\n",
       " 'lag9_street': 'category',\n",
       " 'lag9_street_number': 'category',\n",
       " 'request_street': 'category',\n",
       " 'request_street_number': 'category',\n",
       " 'work_street_name': 'category',\n",
       " 'work_street_number': 'category'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes_df = {}\n",
    "obj_cols = df.select_dtypes(include=['object']).columns[1:].tolist() # [1:] is just to skip the 'date_created'\n",
    "for obj_col in obj_cols:\n",
    "    if(len(df[obj_col].drop_duplicates()) < df.shape[0]/2):\n",
    "        dtypes_df[obj_col] = 'category'\n",
    "dtypes_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the created dictionary as input for the **pd.read_csv** function in the **dtype** parameter. Pandas won't try to infer the column's type and directly try to apply the specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predefined formats data frame memory size: 3225.63 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('*.csv', index_col=0, \n",
    "                 dtype=dtypes_df, parse_dates=['date_created'], infer_datetime_format=True)\n",
    "\n",
    "print('Predefined formats data frame memory size: ' + str(mem_usage(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things I know it doesn't work and other questions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unfortunately, when working with pandas, I know the category data type is not automatically interpreted as factors, like in R. I haven't found a solution for this besides using again lots of memory to convert the category dtypes to strings (pandas will do this when calling pd.get_dummies() anyway)\n",
    "  \n",
    "- If we have to deal with string manipulation, I don't know if the category dtype impact in the performance.  \n",
    "  \n",
    "- In case of nominal data where the categories have an order, *e.g.* 'disagree', 'agree', 'strongly agree', we can instantiate the categorical dtype in a similar way, using one extra parameter of *order = True* [Link here](https://pandas.pydata.org/pandas-docs/stable/categorical.html#controlling-behavior)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
